{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get weekly resolution\n",
    "2. get data in the box of interest\n",
    "3. calculate stats\n",
    "ezpz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masterfiles for each region\n",
    "Each region will have a single netcdf file containing all daily data in the region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bounds(polygon, distance):\n",
    "    return polygon.buffer(distance)\n",
    "\n",
    "def load_polygons(polygon_filepath, buffer_distance=0):\n",
    "\n",
    "    polygons  = gpd.GeoDataFrame.from_file(polygon_filepath)\n",
    "\n",
    "    polygons['geometry'] = polygons['geometry'].apply(expand_bounds, distance=buffer_distance)\n",
    "    return polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for region 00. Skipping...\n",
      "File already exists for region 01. Skipping...\n",
      "File already exists for region 34. Skipping...\n",
      "File already exists for region 02. Skipping...\n",
      "File already exists for region 03. Skipping...\n",
      "File already exists for region 04. Skipping...\n",
      "File already exists for region 05. Skipping...\n",
      "File already exists for region 06. Skipping...\n",
      "File already exists for region 07. Skipping...\n",
      "File already exists for region 08. Skipping...\n",
      "File already exists for region 09. Skipping...\n",
      "File already exists for region 10. Skipping...\n",
      "File already exists for region 11. Skipping...\n",
      "File already exists for region 12. Skipping...\n",
      "Processing Region:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [09:15<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [09:51<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [07:55<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [14:49<00:00,  4.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [09:36<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [09:15<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [08:08<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [08:05<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:05<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:06<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [06:23<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [04:20<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [04:29<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:56<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [06:02<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [06:00<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:27<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:26<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [05:23<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [03:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n",
      "Processing Region:  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 192/192 [02:59<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the dictionary of groups\n",
    "groups = {\n",
    "    1: ['34', '00', '01'],\n",
    "    2: ['02', '03'],\n",
    "    3: ['04', '05', '06'],\n",
    "    4: ['07', '08', '09', '10'],\n",
    "    5: ['11', '12'],\n",
    "    6: ['13', '14', '15'],\n",
    "    7: ['16', '17', '18'],\n",
    "    8: ['19', '20'],\n",
    "    9: ['21', '22', '23'],\n",
    "    10: ['24', '25'],\n",
    "    11: ['26', '27', '28'],\n",
    "    12: ['29', '30', '31'],\n",
    "    13: ['32', '33']\n",
    "}\n",
    "\n",
    "# Iterate over each group in the dictionary\n",
    "def process_group(group, regions):\n",
    "    destination_foldername = f'chla_data_group_{group}'\n",
    "    destination_path = os.path.join('/Volumes/Seagate 5TB/OceanColour Data/', 'regional_chla_data', destination_foldername)\n",
    "    files = [os.path.join(destination_path, f) for f in os.listdir(destination_path) if f.endswith('.nc')]\n",
    "    # files.sort()\n",
    "    files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    # files[:5]\n",
    "    # Load the polygons\n",
    "    polygon_folder  = '/Users/tara/Documents/SJSU/MLML/polygons/pan_gl_regions/coastal_regions'\n",
    "    polygon_file    = 'gl_coastal_regions.shp'\n",
    "\n",
    "    polygon_filepath = os.path.join(polygon_folder, polygon_file)\n",
    "\n",
    "    # expand the bounds of a polygon\n",
    "    buffer_distance = 0.1\n",
    "\n",
    "    polygons = load_polygons(polygon_filepath, buffer_distance)\n",
    "    polygons = polygons[polygons['Region'].isin(regions)]\n",
    "    regions = polygons['Region'].unique()\n",
    "    \n",
    "    polygon_bounds = {}\n",
    "    for region in regions:\n",
    "        polygon = polygons.loc[polygons['Region'] == region, 'geometry'].values[0]\n",
    "        minx, miny, maxx, maxy = polygon.bounds\n",
    "        polygon_bounds[region] = (minx, miny, maxx, maxy)\n",
    "\n",
    "    \n",
    "    for region in regions:\n",
    "        minx, miny, maxx, maxy = polygon_bounds[region]\n",
    "\n",
    "        output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_masterfiles/'\n",
    "        if not os.path.exists(output_fp):\n",
    "            os.makedirs(output_fp)\n",
    "        if os.path.exists(os.path.join(output_fp, f'ds_masterfile_region_{region}.nc')):\n",
    "            print(f\"File already exists for region {region}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(\"Processing Region: \", region)\n",
    "        \n",
    "        # Create an empty list to store the datasets\n",
    "        datasets = []\n",
    "\n",
    "        # Iterate over each file\n",
    "        num_files = len(files)\n",
    "        for i, file in enumerate(tqdm(files, desc=\"Processing files\")):\n",
    "            # Open the file as an xarray dataset\n",
    "            ds = xr.open_dataset(file)\n",
    "            ds = ds.where((ds.lon >= minx) & (ds.lon <= maxx) & (ds.lat >= miny) & (ds.lat <= maxy), drop=True)\n",
    "            ds = ds.where((ds >= 0) & (ds <= 100)) # filter out negative and extreme values\n",
    "            # Append the dataset to the list\n",
    "            datasets.append(ds)\n",
    "            ds.close()\n",
    "\n",
    "        # Concatenate all the datasets along the time dimension\n",
    "        print(\"Concatenating datasets...\\n\")\n",
    "        ds_all = xr.concat(datasets, dim='time')\n",
    "        del datasets\n",
    "\n",
    "        ds_all = ds_all.where((ds_all.lon >= minx) & (ds_all.lon <= maxx) & (ds_all.lat >= miny) & (ds_all.lat <= maxy), drop=True)\n",
    "        ds_all = ds_all.where((ds_all >= 0) & (ds_all <= 100)) # filter out negative and super extreme values\n",
    "\n",
    "        # save ds_all as a .nc file\n",
    "        output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_masterfiles/'\n",
    "        if not os.path.exists(output_fp):\n",
    "            os.makedirs(output_fp)\n",
    "        ds_all.to_netcdf(os.path.join(output_fp, f'ds_masterfile_region_{region}.nc'))\n",
    "        ds_all.close()\n",
    "\n",
    "\n",
    "for group, regions in groups.items():\n",
    "    process_group(group, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create weekly resolution files\n",
    "10x smaller file size to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_masterfiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling data for region 0...\n",
      "Resampling data for region 1...\n",
      "Resampling data for region 2...\n",
      "Resampling data for region 3...\n",
      "Resampling data for region 4...\n",
      "Resampling data for region 5...\n",
      "Resampling data for region 6...\n",
      "Resampling data for region 7...\n",
      "Resampling data for region 8...\n",
      "Resampling data for region 9...\n",
      "Resampling data for region 10...\n",
      "Resampling data for region 11...\n",
      "Resampling data for region 12...\n",
      "Resampling data for region 13...\n",
      "Resampling data for region 14...\n",
      "Resampling data for region 15...\n",
      "Resampling data for region 16...\n",
      "Resampling data for region 17...\n",
      "Resampling data for region 18...\n",
      "Resampling data for region 19...\n",
      "Resampling data for region 20...\n",
      "Resampling data for region 21...\n",
      "Resampling data for region 22...\n",
      "Resampling data for region 23...\n",
      "Resampling data for region 24...\n",
      "Resampling data for region 25...\n",
      "Resampling data for region 26...\n",
      "Resampling data for region 27...\n",
      "Resampling data for region 28...\n",
      "Resampling data for region 29...\n",
      "Resampling data for region 30...\n",
      "Resampling data for region 31...\n",
      "Resampling data for region 32...\n",
      "Resampling data for region 33...\n",
      "Resampling data for region 34...\n"
     ]
    }
   ],
   "source": [
    "def weekly_average(ds):\n",
    "    ds['time'] = pd.to_datetime(ds['time'], origin='unix', unit='D')\n",
    "    \n",
    "    # Resample to weekly data and compute the mean\n",
    "    weekly_ds = ds.resample(time='W').mean()\n",
    "\n",
    "    return weekly_ds\n",
    "\n",
    "def run_resample_data(region):\n",
    "\n",
    "    master_files_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_masterfiles/'\n",
    "    ds_all = xr.open_dataset(os.path.join(master_files_fp, f'ds_masterfile_region_{str(region).zfill(2)}.nc'))\n",
    "\n",
    "    weekly_ds_all = weekly_average(ds_all)\n",
    "    ds_all.close()\n",
    "\n",
    "    # write weekly_ds_all to a netcdf file\n",
    "    output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_weekly_files/'\n",
    "    if not os.path.exists(output_fp):\n",
    "        os.makedirs(output_fp)\n",
    "\n",
    "    weekly_ds_all.to_netcdf(os.path.join(output_fp, f'ds_weekly_file_region_{str(region).zfill(2)}.nc'))\n",
    "    weekly_ds_all.close()\n",
    "\n",
    "for region in range(0, 35):\n",
    "    print(f\"Resampling data for region {region}...\")\n",
    "    run_resample_data(region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get some stats\n",
    "trendline, std error, r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for region 0. Skipping...\n",
      "File already exists for region 1. Skipping...\n",
      "File already exists for region 2. Skipping...\n",
      "File already exists for region 3. Skipping...\n",
      "File already exists for region 4. Skipping...\n",
      "File already exists for region 5. Skipping...\n",
      "File already exists for region 6. Skipping...\n",
      "File already exists for region 7. Skipping...\n",
      "File already exists for region 8. Skipping...\n",
      "File already exists for region 9. Skipping...\n",
      "File already exists for region 10. Skipping...\n",
      "File already exists for region 11. Skipping...\n",
      "File already exists for region 12. Skipping...\n",
      "File already exists for region 13. Skipping...\n",
      "File already exists for region 14. Skipping...\n",
      "File already exists for region 15. Skipping...\n",
      "File already exists for region 16. Skipping...\n",
      "File already exists for region 17. Skipping...\n",
      "File already exists for region 18. Skipping...\n",
      "File already exists for region 19. Skipping...\n",
      "File already exists for region 20. Skipping...\n",
      "File already exists for region 21. Skipping...\n",
      "File already exists for region 22. Skipping...\n",
      "File already exists for region 23. Skipping...\n",
      "File already exists for region 24. Skipping...\n",
      "File already exists for region 25. Skipping...\n",
      "File already exists for region 26. Skipping...\n",
      "File already exists for region 27. Skipping...\n",
      "File already exists for region 28. Skipping...\n",
      "File already exists for region 29. Skipping...\n",
      "Working on region:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 221/221 [16:11<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xarray dataset...\n",
      "Saving results...\n",
      "Working on region:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 210/210 [19:36<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xarray dataset...\n",
      "Saving results...\n",
      "Working on region:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 200/200 [17:38<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xarray dataset...\n",
      "Saving results...\n",
      "Working on region:  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 195/195 [13:02<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xarray dataset...\n",
      "Saving results...\n",
      "Working on region:  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 107/107 [02:50<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xarray dataset...\n",
      "Saving results...\n"
     ]
    }
   ],
   "source": [
    "def compute_row(lat_idx, ds_all):\n",
    "    results = []\n",
    "    for lon_idx in range(len(ds_all.lon)):\n",
    "        y = ds_all['chlor_a'].isel(lat=lat_idx, lon=lon_idx)\n",
    "        valid_mask = np.isfinite(y)\n",
    "        # x = x_ # independent variable: time\n",
    "        x = np.arange(len(ds_all.time))\n",
    "        if np.unique(y[valid_mask]).size > 2: # at least 2 unique values needed to compute a slope\n",
    "            numpts_ = np.unique(x[valid_mask]).size\n",
    "            (slope, intercept), cov = np.polyfit(x[valid_mask], y[valid_mask], 1, cov=True)\n",
    "            stderr = np.sqrt(np.nanmean(np.diag(cov)))\n",
    "\n",
    "            trendline_values = np.polyval((slope, intercept), x)\n",
    "            \n",
    "            correlation_matrix = np.corrcoef(y[valid_mask], trendline_values[valid_mask])\n",
    "            r_squared_value = correlation_matrix[0, 1] ** 2\n",
    "\n",
    "            results.append((lat_idx, lon_idx, numpts_, slope, intercept, stderr, trendline_values, r_squared_value))\n",
    "        else: \n",
    "            results.append((lat_idx, lon_idx, 0, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "    return results\n",
    "\n",
    "def save_stats(region, results, ds_all):\n",
    "    # results.append((lat_idx, lon_idx, numpts_, slope, intercept, stderr, trendline_values, r_squared_value))\n",
    "    # lat_idx, lon_idx, numpts, slope, intercept, stderr, trendline, r_squared = zip(*results)\n",
    "    numpts = np.full_like(ds_all['chlor_a'].isel(time=0), 0) # How many valid points (days) were used to compute the slope for each pixel. use this to fix the slope\n",
    "    slope = np.full((len(ds_all.lat), len(ds_all.lon)), np.nan)\n",
    "\n",
    "    stderr = np.full((len(ds_all.lat), len(ds_all.lon)), np.nan)\n",
    "    r_squared = np.full((len(ds_all.lat), len(ds_all.lon)), np.nan)\n",
    "\n",
    "    trendline = np.full((len(ds_all.time), len(ds_all.lat), len(ds_all.lon)), np.nan)\n",
    "\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    results = [item for sublist in results for item in sublist]\n",
    "\n",
    "    # Assign the results to the arrays\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            lat_idx, lon_idx, numpts_, slope_, intercept, stderr_, trendline_values, r_squared_value = result\n",
    "            numpts[lat_idx, lon_idx] = numpts_\n",
    "            slope[lat_idx, lon_idx] = slope_\n",
    "            stderr[lat_idx, lon_idx] = stderr_\n",
    "            trendline[:, lat_idx, lon_idx] = trendline_values\n",
    "            r_squared[lat_idx, lon_idx] = r_squared_value\n",
    "    del results\n",
    "\n",
    "    print(\"Creating xarray dataset...\")\n",
    "    trendline_ds = xr.Dataset({'trendline': (('time', 'lat', 'lon'), trendline),\n",
    "                            'r_squared': (('lat', 'lon'), r_squared),\n",
    "                            'slope': (('lat', 'lon'), slope),\n",
    "                            'numpts': (('lat', 'lon'), numpts),\n",
    "                            'stderr': (('lat', 'lon'), stderr)},\n",
    "                            coords={'lat': ds_all.lat,\n",
    "                                    'lon': ds_all.lon,\n",
    "                                    'time': ds_all.time})\n",
    "\n",
    "    # Save the dataset to a netCDF file\n",
    "    print(\"Saving results...\")\n",
    "    # output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/test5'\n",
    "    output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_weekly_stats/'\n",
    "    if not os.path.exists(output_fp):\n",
    "        os.makedirs(output_fp)\n",
    "\n",
    "    trendline_ds.to_netcdf(os.path.join(output_fp, f'trendline_weekly_region_{str(region).zfill(2)}.nc'))\n",
    "\n",
    "for region in range(0, 35):\n",
    "    output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_weekly_stats/'\n",
    "    output_file = os.path.join(output_fp, f'trendline_weekly_region_{str(region).zfill(2)}.nc')\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File already exists for region {region}. Skipping...\")\n",
    "        continue\n",
    "    print(\"Working on region: \", region)\n",
    "    weekly_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/regional_weekly_files/'\n",
    "    ds = xr.open_dataset(os.path.join(weekly_fp, f'ds_weekly_file_region_{str(region).zfill(2)}.nc'))\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_row)(lat_idx, ds) for lat_idx in tqdm(range(len(ds.lat)), desc=\"Processing rows\"))\n",
    "    save_stats(region, results, ds)\n",
    "    ds.close()\n",
    "    del results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # log: 04/21/2023 224m to run \n",
    "# ds_all = xr.open_dataset(os.path.join(output_fp, f'ds_masterfile_region_{region}'))\n",
    "# def compute_row(lat_idx):\n",
    "#     results = []\n",
    "#     for lon_idx in range(len(ds_all.lon)):\n",
    "#         y = ds_all['chlor_a'].isel(lat=lat_idx, lon=lon_idx)\n",
    "#         valid_mask = np.isfinite(y)\n",
    "#         x = x_ # independent variable: time\n",
    "#         if np.unique(y[valid_mask]).size > 2: # at least 2 unique values needed to compute a slope\n",
    "#             numpts_ = np.unique(x[valid_mask]).size\n",
    "#             (slope, intercept), cov = np.polyfit(x[valid_mask], y[valid_mask], 1, cov=True)\n",
    "#             stderr = np.sqrt(np.nanmean(np.diag(cov)))\n",
    "\n",
    "#             trendline_values = np.polyval((slope, intercept), x)\n",
    "            \n",
    "#             correlation_matrix = np.corrcoef(y[valid_mask], trendline_values[valid_mask])\n",
    "#             r_squared_value = correlation_matrix[0, 1] ** 2\n",
    "\n",
    "#             results.append((lat_idx, lon_idx, numpts_, slope, intercept, stderr, trendline_values, r_squared_value))\n",
    "#         else: \n",
    "#             results.append((lat_idx, lon_idx, 0, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "#     return results\n",
    "    \n",
    "\n",
    "# results = Parallel(n_jobs=-1)(delayed(compute_row)(lat_idx) for lat_idx in tqdm(range(len(ds_all.lat)), desc=\"Processing rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Creating xarray dataset...\")\n",
    "# trendline_ds = xr.Dataset({'trendline': (('time', 'lat', 'lon'), trendline),\n",
    "#                            'r_squared': (('lat', 'lon'), r_squared),\n",
    "#                            'slope': (('lat', 'lon'), slope),\n",
    "#                            'numpts': (('lat', 'lon'), numpts),\n",
    "#                            'stderr': (('lat', 'lon'), stderr)},\n",
    "#                           coords={'lat': ds_all.lat,\n",
    "#                                   'lon': ds_all.lon,\n",
    "#                                   'time': ds_all.time})\n",
    "\n",
    "# # Save the dataset to a netCDF file\n",
    "# print(\"Saving results...\")\n",
    "# output_fp = '/Volumes/Seagate 5TB/OceanColour Data/statistics/test5'\n",
    "# if not os.path.exists(output_fp):\n",
    "#     os.makedirs(output_fp)\n",
    "\n",
    "# trendline_ds.to_netcdf(os.path.join(output_fp, 'trendline.nc'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenlandchanges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
